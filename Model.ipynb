{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_pipeline import run_pipeline\n",
    "from preprocessing import preprocess\n",
    "from fill_nans import fill_nans\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_feature(data, feature):\n",
    "    \n",
    "    \"\"\" split yfinance data into \n",
    "    \"\"\"\n",
    "    \n",
    "    col_names = data.columns\n",
    "    return data[col_names[[col.split(\".\")[0] == feature for col in col_names]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \n",
    "    \"\"\" Scale data between -1, 1\n",
    "    \"\"\"\n",
    "    \n",
    "    return (2 * (x - np.min(x)) / (np.max(x) - np.min(x))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_target_split(price_data, lag, prediction_interval=0):\n",
    "    \n",
    "    \"\"\" slice data into timeseries classification context\n",
    "    \"\"\"\n",
    "    \n",
    "    features = np.zeros((price_data.shape[0] - lag - prediction_interval, lag))\n",
    "    target = np.zeros(price_data.shape[0] - lag - prediction_interval)\n",
    "\n",
    "    for i in range(lag, price_data.shape[0] - prediction_interval):\n",
    "        features[i - lag] = price_data[i - lag:i]\n",
    "        target[i - lag] = (np.sum(price_data[i:i+prediction_interval+1]) > 0) * 1\n",
    "\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df, lag=25):\n",
    "    \n",
    "    \"\"\" transform dataframe into train test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # get open and close\n",
    "    df_open = get_data_by_feature(df, \"Open\")\n",
    "    df_close = get_data_by_feature(df, \"Close\")\n",
    "    \n",
    "    # get symbols, columns\n",
    "    open_stocks = np.array([df_open.columns[i].split(\".\")[1] for i in range(len(df_open.columns))])\n",
    "    close_stocks = np.array([df_close.columns[i].split(\".\")[1] for i in range(len(df_close.columns))])\n",
    "    column_space = len(df_open.columns)\n",
    "    columns = [df_open.columns[i].split('.')[1] for i in range(column_space)]\n",
    "    \n",
    "    # derive price change from open, close data\n",
    "    price_change_data = np.array([normalize(df_close[df_close.columns[i]].values) - normalize(df_open[df_open.columns[i]].values) for i in range(column_space)])\n",
    "    price_change_df = pd.DataFrame(price_change_data.T, columns=columns)\n",
    "\n",
    "    # iterate over data, save to x and y lists\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(column_space):\n",
    "        data_split = feature_target_split(price_change_data[i], lag)\n",
    "        x.append(data_split[0])\n",
    "        y.append(data_split[1])\n",
    "\n",
    "    # convert datatype and reshape\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    x = np.concatenate([x[i] for i in range(x.shape[0])])\n",
    "    y = np.concatenate([y[i] for i in range(y.shape[0])]).reshape(x.shape[0], 1)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"C:/Users/voyno/Desktop/data/finance/\"\n",
    "training_files = [\"1wk1m_1.csv\", \"1wk1m_2.csv\", \"1wk1m_3.csv\"]\n",
    "testing_files = [\"1wk1m_10.csv\"]\n",
    "\n",
    "# timeseries data for subset of Russel3000 stocks\n",
    "train_df = run_pipeline([dir_path + training_files[i] for i in range(len(training_files))])\n",
    "test_df = run_pipeline(dir_path + testing_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data preparation complete\n",
      "x_train.shape = (3972650, 25) \n",
      "y_train.shape = (3972650, 1)\n",
      "Testing data preparation complete\n",
      "x_test.shape = (1709400, 25) \n",
      "y_test.shape = (1709400, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = prep_data(train_df)\n",
    "print(\"Training data preparation complete\\nx_train.shape =\", x_train.shape, \"\\ny_train.shape =\", y_train.shape)\n",
    "\n",
    "x_test, y_test = prep_data(test_df)\n",
    "print(\"Testing data preparation complete\\nx_test.shape =\", x_test.shape, \"\\ny_test.shape =\", y_test.shape)\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 8s 33ms/step - loss: 0.5885 - acc: 0.6770 - val_loss: 0.6248 - val_acc: 0.6480\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 8s 32ms/step - loss: 0.5662 - acc: 0.6829 - val_loss: 0.6085 - val_acc: 0.6475\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 8s 33ms/step - loss: 0.5634 - acc: 0.6833 - val_loss: 0.6048 - val_acc: 0.6475\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 8s 32ms/step - loss: 0.5622 - acc: 0.6837 - val_loss: 0.6037 - val_acc: 0.6478\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 8s 34ms/step - loss: 0.5615 - acc: 0.6839 - val_loss: 0.6033 - val_acc: 0.6477\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 9s 35ms/step - loss: 0.5609 - acc: 0.6842 - val_loss: 0.6033 - val_acc: 0.6479\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.5605 - acc: 0.6843 - val_loss: 0.6029 - val_acc: 0.6477\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 9s 39ms/step - loss: 0.5602 - acc: 0.6845 - val_loss: 0.6026 - val_acc: 0.6480\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.5599 - acc: 0.6845 - val_loss: 0.6026 - val_acc: 0.6477\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.5597 - acc: 0.6844 - val_loss: 0.6028 - val_acc: 0.6479\n",
      "Mean validation accuracy: 0.6477760016918183\n"
     ]
    }
   ],
   "source": [
    "input_shape=(x_train.shape[1],)\n",
    "node_num = 64\n",
    "dropout_prob = 0.25\n",
    "batch_size=16384\n",
    "epochs=10\n",
    "verbose=1\n",
    "\n",
    "layers = [\n",
    "    Dense(node_num, activation=\"relu\", input_shape=input_shape),\n",
    "    Dropout(dropout_prob),\n",
    "    Dense(node_num, activation=\"relu\"),\n",
    "    Dropout(dropout_prob),\n",
    "    Dense(node_num, activation=\"relu\"),\n",
    "    Dropout(dropout_prob),\n",
    "    Dense(1, activation=\"sigmoid\")]\n",
    "\n",
    "model = Sequential(layers)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"adam\", \n",
    "    metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=verbose)\n",
    "\n",
    "print(\"Mean validation accuracy:\", np.mean(history.history[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
